# 强化学习

强化学习主要包含5个部分：Agent、Enveriment、Observation、Action、Reward

![image-20230719164827953](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230719164827953.png)

我们可以操作个体来做决策，即选择一个合适的动作（Action）。下面的环境代表我们要研究的环境,它有自己的状态模型，我们选择了动作后，环境的状态(State)会变，我们会发现环境状态已经变为St+1,同时我们得到了我们采取动作的延时奖励(Reward)。然后个体可以继续选择下一个合适的动作，然后环境的状态又会变，又有新的奖励值。

那么我们可以整理下这个思路里面出现的强化学习要素：

　　　　第一个是环境的状态S, t时刻环境的状态St是它的环境状态集中某一个状态。

　　　　第二个是个体的动作A, t时刻个体采取的动作At是它的动作集中某一个动作。

　　　　第三个是环境的奖励R,t时刻个体在状态St采取的动作At对应的奖励Rt+1会在t+1时刻得到。

　　　　下面是稍复杂一些的模型要素。

　　　　第四个是个体的策略(policy)π,它代表个体采取动作的依据，即个体会依据策略π来选择动作。最常见的策略表达方式是一个条件概率分布π(a|s), 即在状态s时采取动作a的概率。即π(a|s)=P(a|s).此时概率大的动作被个体选择的概率较高。

　　　　第五个是个体在策略π和状态s时，采取行动后的价值（value），一般用vπ(s)表示。这个价值一般是一个期望函数。虽然当前动作会给一个延时奖励Rt+1,但是光看这个延时奖励是不行的，因为当前的延时奖励高，不代表到了t+1,t+2,...时刻的后续奖励也高。比如下象棋，我们可以某个动作可以吃掉对方的车，这个延时奖励是很高，但是接着后面我们输棋了。此时吃车的动作奖励值高但是价值并不高。因此我们的价值要综合考虑当前的延时奖励和后续的延时奖励。价值函数vπ(s)一般可以表示为下式，不同的算法会有对应的一些价值函数变种，但思路相同。：

​																		$$vπ(s)=Eπ(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_{t=s})$$

　　　　其中γ是第六个模型要素，即奖励衰减因子，在[0，1]之间。如果为0，则是贪婪法，即价值只由当前延时奖励决定，如果是1，则所有的后续状态奖励和当前奖励一视同仁。大多数时候，我们会取一个0到1之间的数字，即当前延时奖励的权重比后续奖励的权重大。

　　　　第七个是环境的状态转化模型，可以理解为一个概率状态机，它可以表示为一个概率模型，即在状态s下采取动作a,转到下一个状态s′的概率，表示为$P^a_{ss′}$。

　　　　第八个是探索率ϵ，这个比率主要用在强化学习训练迭代过程中，由于我们一般会选择使当前轮迭代价值最大的动作，但是这会导致一些较好的但我们没有执行过的动作被错过。因此我们在训练选择最优动作时，会有一定的概率ϵ不选择使当前轮迭代价值最大的动作，而选择其他的动作。

　　　　以上8个就是强化学习模型的基本要素了。当然，在不同的强化学习模型中，会考虑一些其他的模型要素，或者不考虑上述要素的某几个，但是这8个是大多数强化学习模型的基本要素。

demo1: 井字棋、五子棋

![image-20230719212212684](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230719212212684.png)